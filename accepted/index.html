<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Dustin Tran">
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <title>Approximate Inference - Accepted Papers</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/main.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>
<body>
  <div class="container">

    <div class="row" style="padding:20px">

      <div class="hidden-xs col-sm-3 col-md-2">
      </div>

      <div class="col-xs-12 col-sm-12 col-md-9">
        <div class="row" style="margin-bottom:-10px;">
          <div class="col-xs-12 hidden-sm hidden-md hidden-lg">
            <!--
            <a href="http://nips.cc">
              <img src="/img/nips.svg" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
            -->
          </div>
          <div class="hidden-xs">
            <!--
            <a href="http://nips.cc">
              <img src="/img/nips.svg" class="pull-right" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
            -->
          </div>
          <h2>3rd Symposium on<br/>Advances in Approximate Bayesian Inference</h2>
          <p class="lead">
            Virtual Event, January-February, 2021<br>
          </p>
        </div>
      </div>

      <div class="col-xs-12 col-sm-3 col-md-2" id="sidebar" role="navigation">
        <hr>
        <ul class="nav nav-pills nav-stacked">
          <li><a href="../">Home</a></li>
          <!-- <li><a href="../schedule">Schedule</a></li> -->
          <li><a href="./">Call for Papers</a></li>
          <li><a href="../accepted">Accepted Papers</a></li>
          <!-- <li><a href="../program">Program Committee</a></li> -->
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Organizers</li>
            <li><a href="https://scholar.google.co.uk/citations?user=lWDq-ygAAAAJ&hl=en">Matthias Bauer</a></li>
            <li><a href="https://adjidieng.github.io/">Adji Bousso Dieng</a></li>
            <li><a href="http://yingzhenli.net/home/en/">Yingzhen Li</a></li>
            <li><a href="http://dawenl.github.io/">Dawen Liang</a></li>
            <li><a href="http://www.stephanmandt.com/">Stephan Mandt</a></li>
        </ul>
        <!--
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Local Chair</li>
            <li><a href="https://jiaweimtr.github.io/">Eric Jiawei He</a></li>
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Advisory Committee</li>
            <li><a href="http://www.cs.columbia.edu/~blei">David Blei</a></li>
            <li><a href="http://www.stephanmandt.com">Stephan Mandt</a></li>
            <li><a href="http://jamesmc.com">James McInerney</a></li>
            <li><a href="http://dustintran.com/">Dustin Tran</a></li>
        </ul>
      -->
      </div>

      <div class="col-xs-12 col-sm-9 col-md-9">
        <hr>
<!--         <div class="row">
          <h3>Awards</h3>
          <ul>
            <li><b>Best student paper</b>
              <ul>
                <li>Jakub Swiatkowski (University of Warsaw)
                  <br>
                  <i>The k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks</i>
                </li>
              </ul>
            </li>
            <li><b>Best student paper run-ups</b>
              <ul>
                <li>Jiaxin Shi (Tsinghua University)
                  <br>
                  <i>Sparse Orthogonal Variational Inference for Gaussian Processes</i>
                </li>
                <li>Anna Kuzina & Evgenii Egorov (Skoltech)
                  <br>
                  <i>BooVAE: A Scalable Framework for Continual VAE Learning under Boosting Approach</i>
                </li>
              </ul>
            </li>
            <li><b>Best industry paper</b>
              <ul>
                <li>Matthew Hoffman (Google)
                  <br>
                  <i>Langevin Dynamics as Nonparametric Variational Inference</i>
                </li>
              </ul>
            </li>
            <li><b>Best industry paper run-ups</b>
              <ul>
                <li>Iuliia Molchanova (Samsung)
                  <br>
                  <i>Structured semi-implicit variational inference</i>
                </li>
                <li>Roman Novak (Google Brain)
                  <br>
                  <i>Neural Tangents: Fast and Easy Infinite Neural Networks in Python</i>
                </li>
              </ul>
            </li>
          </ul>
        </div> -->
        <div class="row">
          <h3>List of papers</h3>
          <p>Papers can be found in <a href="https://openreview.net/group?id=approximateinference.org/AABI/2021/Symposium">OpenReview</a>, where they are available for archival purposes. This does not
          constitute a proceedings for the symposium.</p>
          <ol>
          <li>
            Roundoff Error in Metropolis-Hastings Accept-Reject Steps
            <br>
            Matthew D. Hoffman
          </li>
          <li>
            Variational Refinement for Importance SamplingUsing the Forward Kullback-Leibler Divergence
            <br>
            Ghassen Jerfel, Serena Lutong Wang, Clara Fannjiang, Katherine A. Heller, Yian Ma, Michael Jordan
          </li>
          <li>
            Marginal Likelihood Gradient for Bayesian Neural Networks
            <br>
            Marcin B. Tomczak, Richard E. Turner
          </li>
          <li>
            Decoupled Sparse Gaussian Processes Components: Separating Decision Making from Data Manifold Fitting
            <br>
            Sebastian Popescu, David J. Sharp, James H. Cole, Ben Glocker
          </li>
          <li>
            Expressive yet Tractable Bayesian Deep Learning via Subnetwork Inference 
            <br>
            Erik Daxberger, Eric Nalisnick, James Urquhart Allingham, Javier Antoran, José Miguel Hernández-Lobato
          </li>
          <li>
            Efficient Calculation of Adversarial Examples for Bayesian Neural Networks
            <br>
            Sina Däubener, Joel Frank, Thorsten Holz, Asja Fischer
          </li>
          <li>
            Gradient-Free Adversarial Attacks for Bayesian Neural Networks
            <br>
            Matthew Yuan, Matthew R. Wicker, Luca Laurenti 
          </li>
          <li>
            Generalized Posteriors in Approximate Bayesian Computation
            <br>
            Sebastian M. Schmon, Patrick W. Cannon, Jeremias Knoblauch
          </li>
          <li>
            On Batch Normalisation for Approximate Bayesian Inference
            <br>
            Jishnu Mukhoti, Puneet K. Dokania, Philip Torr, Yarin Gal
          </li>
          <li>
            Transforming Worlds: Automated Involutive MCMC for Open-Universe Probabilistic Models
            <br>
            George Matheos, Alexander K. Lew, Matin Ghavamizadeh, Stuart Russell, Marco Cusumano-Towner, Vikash Mansinghka
          </li>
          <li>
            Preconditioned training of normalizing flows for variational inference in inverse problems
            <br>
            Ali Siahkoohi, Gabrio Rizzuti, Mathias Louboutin, Philipp Witte, Felix Herrmann
          </li>
          <li>
            Rethinking Function-Space Variational Inference in Bayesian Neural Networks
            <br>
            Tim G. J. Rudner, Zonghao Chen, Yarin Gal
          </li>
          <li>
            Adaptive Strategy for Resetting a Non-stationary Markov Chain during Learning via Joint Stochastic Approximation
            <br>
            Hyunsu Kim, Juho Lee, Hongseok Yang
          </li>
          <li>
            Nested Variational Inference
            <br>
            Heiko Zimmermann, Hao Wu, Babak Esmaeili, Sam Stites, Jan-Willem van de Meent
          </li>
          <li>
            Empirical Evaluation of Biased Methods for Alpha Divergence Minimization
            <br>
            Tomas Geffner, Justin Domke
          </li>
          <li>
            The Gaussian Process Latent Autoregressive Model
            <br>
            Rui Xia, Richard E. Turner, Wessel Bruinsma, William Tebbutt
          </li>
          <li>
            Generative Video Compression as Hierarchical Variational Inference
            <br>
            Ruihan Yang, Yibo Yang, Joseph Marino, Stephan Mandt
          </li>
          <li>
            Learning Discrete State Abstractions With Deep Variational Inference
            <br>
            Ondrej Biza, Robert Platt, Jan-Willem van de Meent, Lawson L. S. Wong
          </li>
          <li>
            Correlated Weights in Infinite Limits of Deep Convolutional Neural Networks
            <br>
            Adrià Garriga-Alonso, Mark van der Wilk
          </li>
          <li>
            VIB is Half Bayes
            <br>
            Alexander A. Alemi, Warren R. Morningstar, Ben Poole, Ian Fischer, Joshua V. Dillon
          </li>
          <li>
            Coupled Gradient Estimators for Discrete Latent Variables
            <br>
            Zhe Dong, Andriy Mnih, George Tucker
          </li>
          <li>
            Approximating the clusters' prior distribution in Bayesian nonparametric models
            <br>
            Daria Bystrova, Julyan Arbel, Guillaume Kon Kam King, François Deslandes
          </li>
          <li>
            Gaussian Process Latent Variable Flows for Massively Missing Data
            <br>
            Vidhi Lalchand, Aditya Ravuri, Neil D. Lawrence
          </li>
          <li>
            Distilling Ensembles Improves Uncertainty Estimates
            <br>
            Zelda E. Mariet, Rodolphe Jenatton, Florian Wenzel, Dustin Tran
          </li>
          <li>
            Evidence Estimation by Kullback-Leibler Integration for Flow-Based Methods
            <br>
            Nikolai Zaki, Théo Galy-Fajou, Manfred Opper
          </li>
          <li>
            Neural Networks as Inter-Domain Inducing Points
            <br>
            Shengyang Sun, Jiaxin Shi, Roger B. Grosse
          </li>
          <li>
            Optimal Transport Couplings of Gibbs Samplers on Partitions for Unbiased Estimation
            <br>
            Brian Trippe, Tin D. Nguyen, Tamara Broderick
          </li>
          <li>
            Exact Langevin Dynamics with Stochastic Gradients
            <br>
            Adrià Garriga-Alonso, Vincent Fortuin
          </li>
          <li>
            Expectation Programming
            <br>
            Tim Reichelt, Adam Golinski, Luke Ong, Tom Rainforth 
          </li>
          <li>
            Posterior Collapse and Latent Variable Non-identifiability
            <br>
            Yixin Wang, John P. Cunningham
          </li>
          <li>
            Slice Sampling Reparameterization Gradients
            <br>
            David M. Zoltowski, Diana Cai, Ryan P. Adams
          </li>
          <li>
            Marginalised Spectral Mixture Kernels with Nested Sampling
            <br>
            Fergus Simpson, Vidhi Lalchand, Carl Edward Rasmussen
          </li>
          <li>
            Neural Linear Models with Functional Gaussian Process Priors
            <br>
            Joe Watson, Jihao Andreas Lin, Pascal Klink, Jan Peters
          </li>
          <li>
            Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes
            <br>
            William Tebbutt, Arno Solin, Richard E. Turner
          </li>
          <li>
            The Gaussian Neural Process
            <br>
            Wessel Bruinsma, James Requeima, Andrew Y. K. Foong, Jonathan Gordon, Richard E. Turner
          </li>
          <li>
            Annealed Stein Variational Gradient Descent
            <br>
            Francesco D'Angelo, Vincent Fortuin
          </li>
          <li>
            A Novel Regression Loss for Non-Parametric Uncertainty Optimization
            <br>
            Joachim Sicking, Maram Akila, Maximilian Pintz, Tim Wirtz, Asja Fischer, Stefan Wrobel
          </li>
          <li>
            Variational Beam Search for Novelty Detection 
            <br>
            Aodong Li, Alex J. Boyd, Padhraic Smyth, Stephan Mandt
          </li>
          <li>
            Gradient Regularisation as Approximate Variational Inference
            <br>
            Ali Unlu, Laurence Aitchison
          </li>
          <li>
            On the Inconsistency of Bayesian Inference for Misspecified Neural Networks
            <br>
            Yijie Zhang, Eric Nalisnick
          </li>
          <li>
            Factorized Gaussian Process Variational Autoencoders 
            <br>
            Metod Jazbec, Michael A. L. Pearce, Vincent Fortuin
          </li>
          <li>
            Understanding Variational Inference in Function-Space
            <br>
            David R. Burt, Sebastian W. Ober, Adrià Garriga-Alonso, Mark van der Wilk
          </li>
          <li>
            Bayesian Neural Network Priors Revisited
            <br>
            Vincent Fortuin, Adrià Garriga-Alonso, Florian Wenzel, Gunnar Ratsch, Richard E Turner, Mark van der Wilk, Laurence Aitchison 
          </li>
          <li>
            Learning Discrete Distributions by Dequantization
            <br>
            Emiel Hoogeboom, Taco Cohen, Jakub M. Tomczak 
          </li>
          <li>
            Conjugate Energy-Based Models
            <br>
            Hao Wu, Babak Esmaeili, Michael L Wick, Jean-Baptiste Tristan, Jan-Willem van de Meent
          </li>
          <li>
            Variational Determinant Estimation with Spherical Normalizing Flows
            <br>
            Simon Arthur Passenheim, Emiel Hoogeboom
          </li>
          <li>
            Gaussian Density Parametrization Flow: Particle and Stochastic Approaches
            <br>
            Théo Galy-Fajou, Valerio Perrone, Manfred Opper
          </li>
          <li>
            Optimal Thinning of MCMC Output 
            <br>
            Marina Riabiz, Wilson Ye Chen, Jon Cockayne, Pawel Swietach, Steven Niederer, Chris Oates
          </li>
          <li>
            Why Cold Posteriors? On the Suboptimal Generalization of Optimal Bayes Estimates
            <br>
            Chen Zeno, Itay Golan, Ari Pakman, Daniel Soudry
          </li>
          <li>
            Bijective-Contrastive Estimation
            <br>
            Jae Hyun Lim, Chin-Wei Huang, Aaron Courville, Christopher Pal 
          </li>
          <li>
            Self-Supervised Variational Auto-Encoders
            <br>
            Ioannis Gatopoulos, Jakub M. Tomczak
          </li>
          <li>
            Generalized Doubly-Reparameterized Gradient Estimators 
            <br>
            Matthias Bauer, Andriy Mnih
          </li>
          <li>
            Sensible Priors for Bayesian Neural Networks through Wasserstein Distance Minimization to Gaussian Processes
            <br>
            Ba-Hien TRAN, Dimitrios Milios, Simone Rossi, Maurizio Filippone
          </li>
          <li>
            Bootstrap Ensembles as Variational Inference
            <br>
            Dimitrios Milios, Pietro Michiardi, Maurizio Filippone
          </li>
          <li>
            Argmax Flows: Learning Categorical Distributions with Normalizing Flows 
            <br>
            Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, Max Welling
          </li>
          <li>
            Scalable Hybrid Hidden Markov Model with Gaussian Process Emission for Sequential Time-series Observations
            <br>
            Yohan Jung, Jinkyoo Park 
          </li>
          <li>
            HWA: Hyperparameters Weight Averaging in Bayesian Neural Networks 
            <br>
            Belhal Karimi, Ping Li 
          </li>
          <li>
            Ensemble sampler for infinite-dimensional inverse problems
            <br>
            Jeremie Coullon, Robert J. Webber
          </li>
          <li>
            i-DenseNets 
            <br>
            Yura Perugachi-Diaz, Jakub M. Tomczak, Sandjai Bhulai 
          </li>
          <li>
            Bayesian Evidential Deep Learning with PAC Regularization
            <br>
            Manuel Haussmann, Sebastian Gerwinn, Melih Kandemir
          </li>
          </ol>
        </div>
      </div>

    </div>

    <hr>

    <footer>
    &nbsp;
    </footer>

  </div>

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/javascript" src="/js/main.js"></script>
</body>
</html>
