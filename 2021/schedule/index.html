<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Dustin Tran">
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <title>Approximate Inference - Schedule</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/main.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>
<body>
  <div class="container">

    <div class="row" style="padding:20px">

      <div class="hidden-xs col-sm-3 col-md-2">
      </div>

      <div class="col-xs-12 col-sm-12 col-md-9">
        <div class="row" style="margin-bottom:-10px;">
          <div class="col-xs-12 hidden-sm hidden-md hidden-lg">
            <!--
            <a href="http://nips.cc">
              <img src="/img/nips.svg" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
            -->
          </div>
          <div class="hidden-xs">
            <!--
            <a href="http://nips.cc">
              <img src="/img/nips.svg" class="pull-right" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
            -->
          </div>
          <h2>3rd Symposium on<br/>Advances in Approximate Bayesian Inference</h2>
          <p class="lead">
            Virtual Event, January-February, 2021<br>
          </p>
        </div>
      </div>

       <div class="col-xs-12 col-sm-3 col-md-2" id="sidebar" role="navigation">
        <hr>
                <ul class="nav nav-pills nav-stacked">
          <li><a href="../">Home</a></li>
          <li><a href="./">Schedule</a></li>
          <li><a href="../call">Call for Papers</a></li>
          <li><a href="../accepted">Accepted Papers</a></li>
          <!-- 
          <li><a href="./program">Program Committee</a></li> 
          -->
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Organizers</li>
            <li><a href="https://scholar.google.co.uk/citations?user=lWDq-ygAAAAJ&hl=en">Matthias Bauer</a></li>
            <li><a href="https://adjidieng.github.io/">Adji Bousso Dieng</a></li>
            <li><a href="http://yingzhenli.net/home/en/">Yingzhen Li</a></li>
            <li><a href="http://dawenl.github.io/">Dawen Liang</a></li>
            <li><a href="http://www.stephanmandt.com/">Stephan Mandt</a></li>
        </ul>
        <!-- <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Local Chair</li>
            <li><a href="https://jiaweimtr.github.io/">Eric Jiawei He</a></li>
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Advisory Committee</li>
            <li><a href="http://www.cs.columbia.edu/~blei">David Blei</a></li>
            <li><a href="http://www.stephanmandt.com">Stephan Mandt</a></li>
            <li><a href="http://jamesmc.com">James McInerney</a></li>
            <li><a href="http://dustintran.com/">Dustin Tran</a></li>
        </ul> -->
      </div>

      <div class="col-xs-12 col-sm-9 col-md-9">
        <hr>
         <div class="row">
          <p>
          This year AABI will be a virtual event consisting of six weekly online seminars held in January through February, 2021. Each seminar will be broadcast via Zoom and in the meantime live-streamed on <a href="https://www.youtube.com/channel/UCZLWLfZ_20mAPKNFLJPlgjQ/featured">AABI Youtube channel</a>. The Zoom registration is free but will be limited. Note that each seminar needs to be <b>registered individually</b>.
          </p>
        </div>

        <div class="row">
          <hr>
          <!--
          <p><strong>
          <span style="color:red;">NOTE:</span> This schedule is subject to change; please check it again once the date of the symposium is closer.
          </strong></p>
          -->
          <!--
          <p><strong>
          The workshop's recording is available on
          <a href="https://www.youtube.com/playlist?list=PLsatQfvo0v3sUhi3ijRme9MyqwLL5EOiG">
          Youtube</a>.
          </strong></p>
          
          <h3>Registration</h3>
          -->
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
<!--           <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="12%">7:30 - 8:20</td>
                <td><strong>Registration + Coffee</strong></td>
              </tr>
            </tbody>
          </table> -->
          <h3>Session 1 </h3>
          <h4>January 13, 2021, 5:00 pm GMT (9:00 am PT)</h4>
          <b><a href="https://www.youtube.com/watch?v=ogYpQbsZ3_E" >Session recording</a> </b>
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <!-- <td width="12%">8:20 - 8:30</td> -->
                <td width="12%">Invited</td>
                <td><strong>Michael I. Jordan</strong>: <em>On the Theory of Gradient-Based Optimization and Sampling: A View from Continuous Time</em> 
                  <br>Abstract: Gradient-based optimization has provided the theoretical and practical foundations on which recent developments in statistical machine learning have reposed. A complementary set of foundations is provided by Monte Carlo sampling, where gradient-based methods have also been leading the way in recent years. We explore links between gradient-based optimization algorithms and gradient-based sampling algorithms. Although these algorithms are generally studied in discrete time, we find that fundamental insights can be obtained more readily if we work in continuous time. Results that I will cover include: (1) there is a counterpart of Nesterov acceleration in the world of Langevin diffusion; (2) Langevin algorithms can converge quickly enough to give logarithmic regret in Bayesian multi-arm bandits; and (3) symplectic integration conserves rates of convergence from continuous time to discrete time.
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <hr>
          <h3>Session 2 </h3>
          <h4>January 20, 2021, 5:00 pm GMT (9:00 am PT)</h4>
          <b><a href="https://www.youtube.com/watch?v=1yk88dH97yE&feature=youtu.be">Session recording</a> </b>
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <!-- <td width="12%">8:20 - 8:30</td> -->
                <td width="12%">Invited</td>
                <td><strong>Maja Rudolph</strong>: <em>Variational Dynamic Mixtures</em>
                  <br>Abstract: Many deep probabilistic time series models struggle with sequences with multi-modal dynamics. While powerful generative models have been developed, we show evidence that the associated approximate inference methods are usually too restrictive and can lead to mode averaging. Mode averaging is problematic in highly multi-modal real world sequences, as it can result in unphysical predictions (e.g., predicted taxi trajectories might run through buildings on the street map if they average between the options to go either right or left). This talk is about variational dynamic mixtures (VDM): a new variational family to infer sequential latent variables with multi-modal dynamics. The VDM approximate posterior at each time step is a mixture density network, whose parameters come from propagating multiple samples through a recurrent architecture. This results in an expressive multi-modal posterior approximation. In an empirical study, we show that VDM outperforms competing approaches on highly multi-modal datasets from different domains.
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">Contributed</td>
                <td><strong>Wessel Bruinsma</strong>: <em>The Gaussian Neural Process</em>
                  <a href="https://openreview.net/forum?id=rzsDn7Vzxf" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://www.youtube.com/watch?v=AIMOmfJInT8" class="btn btn-default btn-xs">Talk</a>
                  <br><strong>Tomas Geffner</strong>: <em>Empirical Evaluation of Biased Methods for Alpha Divergence Minimization</em>
                  <a href="https://openreview.net/forum?id=ihUcld16Mpu" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://youtu.be/BJa5BZMZIrI" class="btn btn-default btn-xs">Talk</a>
                </td>
              </tr>
            </tbody>
          </table>
        </div>

       <div class="row">
          <hr>
          <h3>Session 3 </h3>
          <h4>January 27, 2021, 5:00 pm GMT (9:00 am PT)</h4>
          <b><a href="https://www.youtube.com/watch?v=MuzYHr0mSnA">Session recording</a> </b>
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <!-- <td width="12%">8:20 - 8:30</td> -->
                <td width="12%">Invited</td>
                <td><strong>Justin Domke</strong>: <em>Some Embarrassing Questions about Variational Inference</em>
                  <br>Abstract: Black-box variational inference solves ever-more complex models at ever-higher scale. Yet we can’t answer some basic questions: What’s the best way to estimate the gradient? Can we ensure optimization doesn’t diverge? Is it realistic to optimize other divergences? What’s happening when we integrate Monte Carlo estimators the objective? In this talk I’ll explain why these questions aren’t as innocent as they seem, along with some partial progress towards answers.
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">Contributed</td>
                <td><strong>Emiel Hoogeboom</strong>: <em>Argmax Flows: Learning Categorical Distributions with Normalizing Flows</em> 
                  <a href="https://openreview.net/forum?id=fdsXhAy5Cp" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://youtu.be/bZcVd-8Zu_I" class="btn btn-default btn-xs">Talk</a>
                  <br><strong>Erik Daxberger</strong>: <em>Expressive yet Tractable Bayesian Deep Learning via Subnetwork Inference</em>
                  <a href="https://openreview.net/forum?id=WqMlMsZ07A" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://www.youtube.com/watch?v=HqyhSK_2bHA" class="btn btn-default btn-xs">Talk</a> 
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <hr>
          <h3>Session 4 </h3>
          <h4>February 3, 2021, 5:00 pm GMT (9:00 am PT) </h4>
          <b><a href="https://www.youtube.com/watch?v=gYMhc0Nh1PQ">Session recording</a> </b>
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <!-- <td width="12%">8:20 - 8:30</td> -->
                <td width="12%">Invited</td>
                <td><strong>Fredrik Lindsten</strong>: <em>Sequential Monte Carlo for Approximate Bayesian Inference</em>
                  <br>Abstract: Sequential Monte Carlo (SMC) is a powerful class of methods for approximate Bayesian inference. While originally used mainly for signal processing and inference in dynamical systems, these methods are in fact much more general and can be used to solve many challenging problems in Bayesian statistics and machine learning, even if they lack apparent sequential structure. In this talk I will first discuss the foundations of SMC from a machine learning perspective. We will see that there are two main design choices of SMC: the proposal distribution and the so-called intermediate target distributions, where the latter is often overlooked in practice. Focusing on graphical model inference, I will then show how deterministic approximations, such as variational inference and expectation propagation, can be used to approximate the optimal intermediate target distributions. The resulting algorithm can be viewed as a post-correction of the biases associated with these deterministic approximations. Numerical results show improvements over the baseline deterministic methods as well as over “plain” SMC.
                  <br>The first part of the talk is an introduction to SMC inspired by our recent <a href="https://arxiv.org/abs/1903.04797">Foundations and Trends tutorial</a>. 
                  <br>The second part of the talk, focusing on combining SMC and deterministic approximations for graphical model inference, is based on <a href="https://papers.nips.cc/paper/2018/hash/351869bde8b9d6ad1e3090bd173f600d-Abstract.html">this paper</a>.
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">Contributed</td>
                <td><strong>Jae Hyun Lim</strong>: <em>Bijective-Contrastive Estimation</em> 
                  <a href="https://openreview.net/forum?id=yALYfI1nPlA" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://www.youtube.com/watch?v=zCL8tMU-DGg&feature=youtu.be" class="btn btn-default btn-xs">Talk</a>
                  <br><strong>Nikolai Zaki</strong>: <em>Evidence Estimation by Kullback-Leibler Integration for Flow-Based Methods</em>
                  <a href="https://openreview.net/forum?id=LclKtSfmf9I" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://youtu.be/CXGPIaiTVSc" class="btn btn-default btn-xs">Talk</a>
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <hr>
          <h3>Session 5 </h3>
          <h4>February 10, 2021, 9:00 am GMT (1:00 am PT) </h4>
          <b><a href="https://www.youtube.com/watch?v=K4ndTHJu8sI">Session recording</a> </b>
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <!-- <td width="12%">8:20 - 8:30</td> -->
                <td width="12%">Invited</td>
                <td><strong>Mijung Park</strong>: <em>ABCDP: Approximate Bayesian Computation & Differential Privacy</em>
                  <br>Abstract: We develop a novel approximate Bayesian computation framework, called ABCDP, that produces differentially private posterior samples. Our framework requires minimal modification to existing ABC algorithms. We theoretically analyze the interplay between the noise added for the privacy guarantee and the accuracy of the ABC posterior samples. We apply ABCDP to simulated data as well as privacy-sensitive real data and show the efficacy of the proposed framework.
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">Contributed</td>
                <td><strong>Hao Wu</strong>: <em>Conjugate Energy-Based Models</em>
                  <a href="https://openreview.net/forum?id=4k58RmAD02" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://youtu.be/1SMaKykE4oE" class="btn btn-default btn-xs">Talk</a>
                  <br><strong>William Tebbutt</strong>: <em>Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes</em>
                  <a href="https://openreview.net/forum?id=Ctq5FVu8KX" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://youtu.be/5_rwTtYkV5A" class="btn btn-default btn-xs">Talk</a>
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <hr>
          <h3>Session 6 </h3>
          <h4>February 17, 2021, 5:00 pm GMT (9:00 am PT) </h4>
          <b><a href="https://www.youtube.com/watch?v=1UTCA3a7h_I">Session recording</a> </b>
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <!-- <td width="12%">8:20 - 8:30</td> -->
                <td width="12%">Invited</td>
                <td><strong>Jascha Sohl-Dickstein</strong>: <em>Infinite Width Bayesian Neural Networks</em>
                  <br>Abstract: As neural networks become wider their accuracy improves, and their behavior becomes easier to analyze theoretically. I will give an introduction to a rapidly growing body of work which examines the distribution over functions induced by infinitely wide, randomly initialized, neural networks. Core results that I will discuss include: that the distribution over functions computed by a wide neural network often corresponds to a Gaussian process with a particular compositional kernel; that the predictions of wide neural networks are linear in their parameters throughout training; that this perspective enables analytic predictions for how trainability of finite width networks depends on hyperparameters and architecture; and finally that results on infinite width networks can enable efficient posterior sampling from finite width Bayesian networks. These results provide for surprising capabilities -- for instance, the evaluation of test set predictions which would come from an infinitely wide Bayesian or gradient-descent-trained trained neural network without ever instantiating a neural network, or the rapid training of 10,000+ layer convolutional networks. I will argue that this growing understanding of neural networks in the limit of infinite width is foundational for future theoretical and practical understanding of deep learning.
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">Contributed</td>
                <td><strong>Matthew Hoffman</strong>: <em>Roundoff Error in Metropolis-Hastings Accept-Reject Steps</em>
                  <a href="https://openreview.net/forum?id=aTAP4rQBrAx" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://youtu.be/bit6webKLIo" class="btn btn-default btn-xs">Talk</a>
                  <br><strong>Alex Alemi</strong>: <em>VIB is Half Bayes</em>
                  <a href="https://openreview.net/forum?id=97FiVYw4mrF" class="btn btn-default btn-xs">Paper</a>
                  <a href="https://www.youtube.com/watch?v=JGDAZ4joUX8&feature=youtu.be" class="btn btn-default btn-xs">Talk</a>
                </td>
              </tr>
            </tbody>
          </table>
        </div>
    </div>

    <hr>

    <footer>
    &nbsp;
    </footer>

  </div>

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/javascript" src="/js/main.js"></script>
</body>
</html>
