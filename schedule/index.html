<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Dustin Tran">
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <title>Approximate Inference - Schedule</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/main.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>
<body>
  <div class="container">

    <div class="row" style="padding:20px">

      <div class="hidden-xs col-sm-3 col-md-2">
      </div>

      <div class="col-xs-12 col-sm-12 col-md-9">
        <div class="row" style="margin-bottom:-10px;">
          <div class="col-xs-12 hidden-sm hidden-md hidden-lg">
            <!--
            <a href="http://nips.cc">
              <img src="/img/nips.svg" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
            -->
          </div>
          <div class="hidden-xs">
            <!--
            <a href="http://nips.cc">
              <img src="/img/nips.svg" class="pull-right" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
            -->
          </div>
          <h2>4th Symposium on<br/>Advances in Approximate Bayesian Inference</h2>
          <p class="lead">
            Virtual Event, February 1st and 2nd, 2022<br>
          </p>
        </div>
      </div>

       <div class="col-xs-12 col-sm-3 col-md-2" id="sidebar" role="navigation">
        <hr>
        <ul class="nav nav-pills nav-stacked">
          <li><a href="../">Home</a></li>
          <li><a href=".">Schedule</a></li>
          <li><a href="../call">Call for Papers</a></li>
          <li><a href="../accepted">Accepted Papers</a></li>
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Organizers</li>
            <li><a href="https://fortuin.github.io/">Vincent Fortuin</a></li>
            <li><a href="https://karaletsos.com/">Theofanis Karaletsos</a></li>
            <li><a href="http://yingzhenli.net/home/en/">Yingzhen Li</a></li>
            <li><a href="http://www.stephanmandt.com/">Stephan Mandt</a></li>
            <li><a href="https://ruqizhang.github.io/">Ruqi Zhang</a></li>
        </ul>
      </div>

      <div class="col-xs-12 col-sm-9 col-md-9">
        <hr>
         <div class="row">
          <p>
          This year AABI will be a virtual event consisting of two days online seminars held in February 1st-2nd, 2022. Each seminar will be broadcast via Zoom and in the meantime live-streamed on the <a href="https://www.youtube.com/channel/UC4BwJ3lMd2ATpVytOGckXqg">AABI 2022 Youtube channel</a>. The <a href="https://uci.zoom.us/webinar/register/WN_QI9xdQU1Sp6gmDgl152LUw">Zoom registration</a> is free but will be limited.
          </p>
        </div>

        <div class="row">
          <hr>
          <!--
          <p><strong>
          <span style="color:red;">NOTE:</span> This schedule is subject to change; please check it again once the date of the symposium is closer.
          </strong></p>
          -->
          <!--
          <p><strong>
          The workshop's recording is available on
          <a href="https://www.youtube.com/playlist?list=PLsatQfvo0v3sUhi3ijRme9MyqwLL5EOiG">
          Youtube</a>.
          </strong></p>
          -->
    
          <h3>Day 1 (Feb 1st)</h3>
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="12%">4:00-4:40 pm GMT</td>
                <td width="8%">Invited</td>
                <td>
                  <strong>Aki Vehtari</strong>: <em>Pareto-k as practical pre-asymptotic diagnostic of Monte Carlo estimates</em>
                  <a href="https://youtu.be/U_EbJMMVdAU?t=214" class="btn btn-default btn-xs">Video</a>
                  <br>Abstract: I discuss the use of the Pareto-k diagnostic as a simple and practical approach for estimating pre-asymptotic reliability of Monte Carlo estimates, with examples in importance sampling, stochastic optimization, and variational inference.

                <!-- <br>
                <a href="/2019/schedule/Titsias2019.pdf"
                class="btn btn-default btn-xs">Slides</a> -->
                </td>
              </tr>
              <tr>
                <td width="12%">4:40-5:00 pm GMT</td>
                <td width="8%">Contributed</td>
                <td>
                <em>Bayesian Learning via Neural Schrödinger-Föllmer Flows
                </em>
                <a href="https://youtu.be/U_EbJMMVdAU?t=2633" class="btn btn-default btn-xs">Video</a>
                <!-- <br> -->
                <!-- <a href="/2019/schedule/Swiatkowski2019.pdf"
                class="btn btn-default btn-xs">Slides</a> -->
                </td>
              </tr>
              <tr>
                <td width="12%">5:00-5:40 pm GMT</td>
                <td width="8%">Invited</td>
                <td>
                  <strong>Pavel Izmailov</strong>: <em>What Are Bayesian Neural Network Posteriors Really Like?</em>
                  <a href="https://youtu.be/U_EbJMMVdAU?t=3668" class="btn btn-default btn-xs">Video</a>
                  <br>Abstract: The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a "cold posterior" effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; we demonstrate, explain and provide remedies for this effect; (6) while cheaper alternatives such as deep ensembles and SGMCMC methods can provide good generalization, they provide distinct predictive distributions from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.

                </td>
              </tr>
            </tbody>
          </table>
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">5:40-6:00 pm GMT</td>
                <td><strong>Coffee Break</strong></td>
              </tr>
            </tbody>
          </table>
        
        <table class="table" style="margin-bottom:0px;">
          <tbody>
            <tr>
              <td width="12%">6:00-6:40 pm GMT</td>
              <td width="8%">Invited</td>
              <td>
                <strong>Lester Mackey</strong>: <em>Kernel Thinning and Stein Thinning</em>
                <a href="https://youtu.be/U_EbJMMVdAU?t=7305" class="btn btn-default btn-xs">Video</a>
                <br>Abstract: This talk will introduce two new tools for summarizing a probability distribution more effectively than independent sampling or standard Markov chain Monte Carlo thinning:
                <!-- <br> -->
                1. Given an initial n point summary (for example, from independent sampling or a Markov chain), kernel thinning finds a subset of only square-root n points with comparable worst-case integration error across a reproducing kernel Hilbert space.
                <!-- <br> -->
                2. If the initial summary suffers from biases due to off-target sampling, tempering, or burn-in, Stein thinning simultaneously compresses the summary and improves the accuracy by correcting for these biases.
                <!-- <br> -->
                These tools are especially well-suited for tasks that incur substantial downstream computation costs per summary point like organ and tissue modeling in which each simulation consumes 1000s of CPU hours.
              </td>
            </tr>
            <tr>
              <td width="12%">6:40-7:00 pm GMT</td>
              <td width="8%">Contributed</td>
              <td>
              <em>Linearised Laplace Inference in Networks with Normalisation Layers and the Neural g-Prior
              </em>
              <a href="https://youtu.be/U_EbJMMVdAU?t=9885" class="btn btn-default btn-xs">Video</a>
              <!-- <br> -->
              <!-- <a href="/2019/schedule/Swiatkowski2019.pdf"
              class="btn btn-default btn-xs">Slides</a> -->
              </td>
            </tr>
            <tr>
              <td width="12%">7:00-7:20 pm GMT</td>
              <td width="8%">Contributed</td>
              <td>
              <em>Sampling with Mirror Stein Operators
              </em>
              <a href="https://youtu.be/U_EbJMMVdAU?t=10994" class="btn btn-default btn-xs">Video</a>
              <!-- <br> -->
              <!-- <a href="/2019/schedule/Swiatkowski2019.pdf"
              class="btn btn-default btn-xs">Slides</a> -->
              </td>
            </tr>
          </tbody>
        </table>
        <table class="table">
          <tbody>
            <tr>
              <td width="12%">7:30-9:00 pm GMT</td>
              <td><strong>Poster Session</strong>
              <br>
              Please join <a href="https://gather.town/app/0nkfsvj18xh9Tk6U/AABI-2022">Gathertown</a> here.
              </td>
            </tr>
          </tbody>
        </table>
      </div>

    <div class="row">
      <h3>Day 2 (Feb 2nd)</h3>
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="12%">2:00-2:40 pm GMT</td>
                <td width="8%">Invited</td>
                <td>
                  <strong>Pierre Alquier</strong>: <em>What can we expect from PAC-Bayes bounds?</em>
                  <a href="https://youtu.be/8MWOhYg89fY?t=143" class="btn btn-default btn-xs">Video</a>
                  <br>Abstract: PAC-Bayes bounds were developed to understand the generalization ability of
                  randomized predictors, ensemble methods and Bayesian machine learning algorithms.
                  However, a naive application of these bounds to sophisticated algorithms usually leads
                  to vacuous generalization certificates. Many improvements were proposed in the past
                  few years to obtain non-vacuous guarantees. Recently some very tight certificates were
                  obtained. However, some ideas beyond these improvements are not totally understood.
                  
                  In this talk, I will illustrate with very simple examples what can go very wrong with PAC-Bayes
                  bounds. I will then discuss how to fix these issues by choosing better priors. This will
                  also highlight a deep connection to the recent literature on Mutual Information bounds.
                  In some models, this leads to a clear view of how tight the certificates obtained from
                  PAC-Bayes bounds can be.
                <!-- <br>
                <a href="/2019/schedule/Titsias2019.pdf"
                class="btn btn-default btn-xs">Slides</a> -->
                </td>
              </tr>
              <tr>
                <td width="12%">2:40-3:00 pm GMT</td>
                <td width="8%">Contributed</td>
                <td>
                <em>Sliced Wasserstein Variational Inference
                </em>
                <a href="https://youtu.be/8MWOhYg89fY?t=2415" class="btn btn-default btn-xs">Video</a>
                <!-- <br> -->
                <!-- <a href="/2019/schedule/Swiatkowski2019.pdf"
                class="btn btn-default btn-xs">Slides</a> -->
                </td>
              </tr>
              <tr>
                <td width="12%">3:00-3:40 pm GMT</td>
                <td width="8%">Invited</td>
                <td>
                  <strong>Yixin Wang</strong>: <em>Posterior Collapse and Latent Variable Non-identifiability</em>
                  <a href="https://youtu.be/8MWOhYg89fY?t=3768" class="btn btn-default btn-xs">Video</a>
                <br>Abstract: Variational autoencoders model high-dimensional data by positing low-dimensional latent variables that are mapped through a flexible distribution parametrized by a neural network. Unfortunately, variational autoencoders often suffer from posterior collapse: the posterior of the latent variables is equal to its prior, rendering the variational autoencoder useless as a means to produce meaningful representations. Existing approaches to posterior collapse often attribute it to the use of neural networks or optimization issues due to variational approximation. In this paper, we show that posterior collapse is a problem of latent variable non-identifiability. We prove that the posterior collapses if and only if the latent variables are non-identifiable in the generative model. This fact implies that posterior collapse is not a phenomenon specific to the use of flexible distributions or approximate inference. Rather, it can occur in classical probabilistic models even with exact inference, which we also demonstrate. Based on these results, we propose a class of identifiable variational autoencoders, deep generative models which enforce identifiability without sacrificing flexibility. This model class resolves the problem of latent variable non-identifiability by leveraging bijective Brenier maps and parameterizing them with input convex neural networks, without special variational inference objectives or optimization tricks. Across synthetic and real datasets, identifiable variational autoencoders outperform existing methods in mitigating posterior collapse and providing meaningful representations of the data.

                This is joint work with David Blei and John Cunningham. 
                
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">3:40-4:00 pm GMT</td>
                <td><strong>Coffee Break</strong></td>
              </tr>
            </tbody>
          </table>
        
        <table class="table" style="margin-bottom:0px;">
          <tbody>
            <tr>
              <td width="12%">4:00-4:40 pm GMT</td>
              <td width="8%">Invited</td>
              <td>
                <strong>Kunal Talwar</strong>: <em>Privacy Amplification by Shuffling</em>
                <a href="https://youtu.be/8MWOhYg89fY?t=7356" class="btn btn-default btn-xs">Video</a>
                <br>Abstract: Traditionally, Differential Privacy has been studied in two models: the local model which requires little trust assumptions, and the central model which needs a trusted curator and can achieve better utility. This talk will be about recent works showing that random shuffling amplifies differential privacy guarantees of locally randomized data. Such amplification implies substantially stronger privacy guarantees for systems in which data is contributed anonymously and allows us to get the strong utility of the central model without a trusted curator, as long as we can implement a secure shuffler. We show that random shuffling of $n$ data records that are input to $\eps_0$-differentially private local randomizers results in an $(O((\sqrt{\frac{e^{\eps_0}\log(1/\delta)}{n}}), \delta)$-differentially private algorithm. This significantly improves over previous work and achieves the asymptotically optimal dependence in $\eps_0$. Our result is based on a new approach that is simpler than previous work and extends to approximate differential privacy with nearly the same guarantees. Importantly, our work also yields an algorithm for deriving tighter bounds on the resulting $\eps$ and $\delta$ as well as R\'enyi differential privacy guarantees. We show numerically that our algorithm gets to within a small constant factor of the optimal bound. As a direct corollary of our analysis we derive a simple and nearly optimal algorithm for frequency estimation in the shuffle model of privacy. We also observe that our result implies the first asymptotically optimal privacy analysis of noisy stochastic gradient descent that applies to sampling without replacement.
              <!-- <br>
              <a href="/2019/schedule/Titsias2019.pdf"
              class="btn btn-default btn-xs">Slides</a> -->
              </td>
            </tr>
            <tr>
              <td width="12%">4:40-5:00 pm GMT</td>
              <td width="8%">Contributed</td>
              <td>
              <em>Deep Reference Priors: What is the best way to pretrain a model?
              </em>
              <a href="https://youtu.be/8MWOhYg89fY?t=9936" class="btn btn-default btn-xs">Video</a>
              <!-- <br> -->
              <!-- <a href="/2019/schedule/Swiatkowski2019.pdf"
              class="btn btn-default btn-xs">Slides</a> -->
              </td>
            </tr>
            <tr>
              <td width="12%">5:00-5:20 pm GMT</td>
              <td width="8%">Contributed</td>
              <td>
              <em>Fast Finite Width Neural Tangent Kernel
              </em>
              <a href="https://youtu.be/8MWOhYg89fY?t=10984" class="btn btn-default btn-xs">Video</a>
              <!-- <br> -->
              <!-- <a href="/2019/schedule/Swiatkowski2019.pdf"
              class="btn btn-default btn-xs">Slides</a> -->
              </td>
            </tr>
          </tbody>
        </table>
        <table class="table">
          <tbody>
            <tr>
              <td width="12%">5:30-6:30 pm GMT</td>
              <td><strong>Panel</strong>
                <a href="https://youtu.be/8MWOhYg89fY?t=12790" class="btn btn-default btn-xs">Video</a>
                <br>
                Michael Betancourt, Jan-Willem van de Meent, Chris J. Maddison, Karen Ullrich, Justin Domke
                <br>
                Moderator: Stephan Mandt
                <br>
            </tr>
          </tbody>
        </table>
    </div>

    <hr>

    <footer>
    &nbsp;
    </footer>

  </div>

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/javascript" src="/js/main.js"></script>
</body>
</html>
