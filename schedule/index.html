<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Dustin Tran">
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <title>Approximate Inference - Schedule</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/main.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>
<body>
  <div class="container">

    <div class="row" style="padding:20px">

      <div class="hidden-xs col-sm-3 col-md-2">
      </div>

      <div class="col-xs-12 col-sm-12 col-md-9">
        <div class="row" style="margin-bottom:-10px;">
          <a href="http://nips.cc">
            <img src="/img/nips.svg" class="pull-right" style="margin:0px -90px
            0 0; height:140px; width:250px;">
          </a>
          <h2>Advances in Approximate Bayesian Inference</h2>
          <p class="lead">
            NIPS 2015 Workshop;
            December 11, 2015<br>
            Room 513 ab,
            Palais des Congrès de Montréal,
            Montréal, Canada<br>
          </p>
        </div>
      </div>

       <div class="col-xs-12 col-sm-3 col-md-2" id="sidebar" role="navigation">
        <hr>
        <ul class="nav nav-pills nav-stacked">
          <li><a href="/">Home</a></li>
          <li><a href="/schedule">Schedule</a></li>
          <li><a href="/call">Call for Papers</a></li>
          <li><a href="/accepted">Accepted Papers</a></li>
          <li><a href="/program">Program Committee</a></li>
        </ul>
      </div>

      <div class="col-xs-12 col-sm-9 col-md-9">
        <div class="row">
          <hr>
          <h3>Session 1</h3>
          <h4>Chair: Tamara Broderick</h4>
          <table class="table">
            <tbody>
              <tr>
                <td width="20%">8:30 - 8:35</td>
                <td>Introduction</td>
              </tr>
              <tr>
                <td>8:35 - 9:00</td>
                <td>Rajesh Ranganath: <em>Challenges to variational inference: Optimization, automation, and accuracy</em></td>
              </tr>
              <tr>
                <td>9:00 - 9:15</td>
                <td>
                Theophane Weber:
                <em>Reinforced Variational Inference</em>
                <a href="/accepted">(contributed)</a>
                </td>
              </tr>
              <tr>
                <td>9:15 - 10:00</td>
                <td><strong>Panel</strong>: <em>Tricks of the Trade</em>
                <br>
                Matt Hoffman, Danilo Rezende, David Duvenaud, Alp
                Kucukelbir, Stephan Mandt, Michael Betancourt
                <br>
                Moderator: Tamara Broderick
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Session 2</h3>
          <h4>Chair: Alp Kucukelbir</h4>
          <table class="table">
            <tbody>
              <tr>
                <td width="20%">10:30 - 10:55</td>
                <td>Andrea Montanari: <em>Approximate inference with
                semidefinite relaxations</em></td>
              </tr>
              <tr>
                <td>10:55 - 11:10</td>
                <td>
                Rajesh Ranganath:
                <em>Hierarchical Variational Models</em>
                <a href="/accepted">(contributed)</a>
                </td>
              </tr>
              <tr>
                <td>11:10 - 11:30</td>
                <td>Poster spotlights
                <ul>
                <br>
                Jose Miguel Hernandez-Lobato:
                <em>Black-box α-divergence Minimization</em>
                <br>
                Brooks Paige:
                <em>Inference Networks for Graphical Models</em>
                <br>
                Ryan Giordano:
                <em>Robust Inference with Variational Bayes</em>
                <br>
                Philip Bachman:
                <em>Training Deep Generative Models: Variations on a Theme</em>
                </ul>
                </td>
              </tr>
              <tr>
                <td>11:30 - 1:00</td>
                <td>Poster session</td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Session 3</h3>
          <h4>Chair: Dustin Tran, James McInerney</h4>
          <table class="table">
            <tbody>
              <tr>
                <td width="20%">2:40 - 3:05</td>
                <td>James Hensman: <em>A framework for variational
                inference in Gaussian process models</em></td>
              </tr>
              <tr>
                <td>3:05 - 3:20</td>
                <td>
                Daniel Hernandez-Lobato:
                <em>Stochastic Expectation Propagation for Large Scale Gaussian
                Process Classification</em>
                <a href="/accepted">(contributed)</a>
                </td>
              </tr>
              <tr>
                <td>3:20 - 3:35	</td>
                <td>
                Cedric Archambeau:
                <em>Incremental Variational Inference for Latent Dirichlet
                Allocation</em>
                <a href="/accepted">(contributed)</a>
                </td>
              </tr>
              <tr>
                <td>3:35 - 4:00</td>
                <td>Emily Fox: <em>Variational inference for
                large-scale and streaming sequential data</em></td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Session 4</h3>
          <h4>Chair: Stephan Mandt</h4>
          <table class="table">
            <tbody>
              <tr>
                <td width="20%">4:30 - 4:55</td>
                <td>Manfred Opper: <em>Approximate inference for Ising models with random couplings</em></td>
              </tr>
              <tr>
                <td>4:55 - 6:00 </td>
                <td><strong>Panel</strong>: <em>On the Foundations and
                Future of Approximate Inference</em>
                <br>
                Max Welling, Yee Whye Teh, Andrew Gelman, Steve
                MacEachern, Ulrich Paquet
                <br>
                Moderator: David Blei
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Abstracts</h3>
          <hr>
          <h4><a href="http://www.cs.princeton.edu/~rajeshr/">Rajesh
          Ranganath</a> (Princeton University)</h4>
          <h4>Challenges to variational inference: Optimization,
          automation, and accuracy</h4>
          <p>
          <strong>Abstract</strong>.
          Variational inference is experiencing a resurgence. In
          recent years, researchers have expanded the scope of
          variational inference to more complex Bayesian models,
          reduced its computational cost, and developed new
          theoretical insights. However, several challenges remain. In
          this talk I will discuss some of our recent work on these
          challenges: addressing local optima with tempering,
          automating variational inference in Stan, and constructing
          richer variational approximations with variational models.
          Along the way, I'll discuss some research questions related
          to these areas.
          <br>
          Joint work with Dave Blei, Alp Kucukelbir, Stephan Mandt,
          James McInerney, and Dustin Tran.
          </p>

          <h4><a href="http://web.stanford.edu/~montanar/">Andrea
          Montanari</a> (Stanford University)</h4>
          <h4>Approximate inference with semidefinite relaxations</h4>
          <p>
          <strong>Abstract</strong>.
          Statistical inference problems arising within signal
          processing, data mining, and machine learning naturally give
          rise to hard combinatorial optimization problems.  These
          problems become intractable when the dimensionality of the
          data is large, as is often the case for modern datasets. A
          popular idea is to construct convex relaxations of these
          combinatorial problems, which can be solved efficiently for
          large scale datasets.
          <!--</p>-->
          <!--<p> -->
          Semidefinite programming (SDP) relaxations are among the
          most powerful methods in this family, and are surprisingly
          well-suited for a broad range of problems where data take
          the form of matrices or graphs. It has been observed several
          times that, when the `statistical noise' is small enough,
          SDP relaxations correctly detect the underlying
          combinatorial structures.
          <!--</p>-->
          <!--<p> -->
          I will present a few asymptotically exact predictions for
          the `detection thresholds' of SDP relaxations, with
          applications to synchronization and community detection.
          <br>
          Joint work with Adel Javanmard, Federico Ricci-Tersenghi, and
          Subhabrata Sen.
          </p>

          <h4><a href="http://jameshensman.github.io">James Hensman</a>
          (Lancaster University)</h4>
          <h4>A framework for variational inference in Gaussian
          process models</h4>
          <p>
          <strong>Abstract</strong>.
          Gaussian process models are widely used in statistics and
          machine learning. There are three key challenges to
          inference that might be tackled using variational methods:
          inference over the latent function values when the
          likelihood is non-Gaussian; scaling the computation to large
          datasets; inference over the kernel-parameters. I'll show
          how the variational framework can be used to tackle any or
          all of these challenges. In particular, I'll share recent
          insights which allow us to distinguish the variational
          stochastic process approximation, improving on the idea of a
          low-rank approximation to the posterior. To do this we show
          that it's possible to minimize the KL divergence between the
          true and approximate stochastic processes.
          <br>
          Joint work with Alexander G. de G. Matthews, Nicolo Fusi,
          Maurizio Filippone, Neil D. Lawrence, and Zoubin Ghahramani.
          </p>

          <h4><a href="http://www.stat.washington.edu/~ebfox/">Emily Fox</a>
          (University of Washington)</h4>
          <h4>Variational inference for large-scale and streaming
          sequential data</h4>
          <p>
          <strong>Abstract</strong>.
          Variational inference algorithms have proven successful for
          Bayesian analysis in large data settings, with recent
          advances using stochastic variational inference (SVI).
          However, such methods have largely been studied in
          independent data settings. We develop an SVI algorithm to
          learn the parameters of hidden Markov models (HMMs). The
          challenge in applying stochastic optimization in this
          setting arises from dependencies in the chain, which must be
          broken to consider minibatches of observations. We propose
          an algorithm that harnesses the memory decay of the chain to
          adaptively bound errors arising from edge effects. We
          demonstrate the effectiveness of our algorithm on synthetic
          experiments and a large genomics dataset where a batch
          algorithm is computationally infeasible. We will also
          briefly discuss the streaming data scenario.
          <br>
          Joint work with Nick Foti, Jason Xu, Dillon Laird, and Alex
          Tank.
          </p>

          <h4><a
          href="http://www.eecs.tu-berlin.de/menue/einrichtungen/professuren/professorinnen/opper">Manfred
          Opper</a> (Technische Universität Berlin)</h4>
          <h4>Approximate inference for Ising models with random couplings</h4>
          <p>
          <strong>Abstract</strong>.
          Assume that we try to compute the expectations of variables
          in an Ising model with pairwise interactions. Interpreting
          this model as a latent Gaussian variable model, an EP style
          algorithm would seem to be a possible solution. However, the
          necessary matrix inversions would make this approach
          numerically unfeasible when the model is large. Things
          simplify when it is known that the couplings in the model
          are drawn at random from an invariant random matrix
          distribution. In this case, the fixed points of EP are
          solutions to so—called TAP equations studied in statistical
          physics for which some costly matrix terms are ‘averaged
          out’. But how should one solve such equations? In this
          talk I will propose an algorithmic approach for this task
          and analyse its properties for large systems using dynamical
          functional methods of statistical physics. I will finally
          present results for different random matrix ensembles.
          <br>
          Joint work with Burak Cakmak and Ole Winther.
          </p>
        </div>

      </div>

    </div>

    <hr>

    <footer>
    &nbsp;
    </footer>

  </div>

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/javascript" src="/js/main.js"></script>
</body>
</html>
