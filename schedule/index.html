<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Dustin Tran">
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <title>Approximate Inference - Schedule</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/main.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>
<body>
  <div class="container">

    <div class="row" style="padding:20px">

      <div class="hidden-xs col-sm-3 col-md-2">
      </div>

      <div class="col-xs-12 col-sm-12 col-md-9">
        <div class="row" style="margin-bottom:-10px;">
          <div class="col-xs-12 hidden-sm hidden-md hidden-lg">
            <a href="http://nips.cc">
              <img src="/img/nips.svg" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
          </div>
          <div class="hidden-xs">
            <a href="http://nips.cc">
              <img src="/img/nips.svg" class="pull-right" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
          </div>
          <h2>Advances in Approximate Bayesian Inference</h2>
          <p class="lead">
            NIPS 2016 Workshop;
            December 9, 2016<br>
            Room 112,
            Centre Convencions Internacional Barcelona,
            Barcelona, Spain<br>
          </p>
        </div>
      </div>

       <div class="col-xs-12 col-sm-3 col-md-2" id="sidebar" role="navigation">
        <hr>
        <ul class="nav nav-pills nav-stacked">
          <li><a href="../">Home</a></li>
          <li><a href=".">Schedule</a></li>
          <li><a href="../call">Call for Papers</a></li>
          <li><a href="../accepted">Accepted Papers</a></li>
          <li><a href="../program">Program Committee</a></li>
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Organizers</li>
          <li><a href="http://tamarabroderick.com">Tamara Broderick</a>
          </li>
          <li><a href="http://www.stephanmandt.com">Stephan Mandt</a>
          </li>
          <li><a href="http://jamesmc.com">James McInerney</a>
          </li>
          <li><a href="http://dustintran.com">Dustin Tran</a>
          </li>
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Advisory Committee</li>
          <li><a href="http://www.cs.columbia.edu/~blei">David Blei</a>
          </li>
          <li><a href="http://www.cs.ubc.ca/~murphyk">Kevin Murphy</a>
          </li>
          <li><a href="http://www.stat.columbia.edu/~gelman">Andrew Gelman</a>
          </li>
          <li><a href="https://people.eecs.berkeley.edu/~jordan">Michael I. Jordan</a>
          </li>
        </ul>
      </div>

      <div class="col-xs-12 col-sm-9 col-md-9">
        <div class="row">
          <hr>
          <h3>Session 1</h3>
          <h4>Chair: Dustin Tran</h4>
          <table class="table">
            <tbody>
              <tr>
                <td width="20%">8:30 - 8:35</td>
                <td>Introduction</td>
              </tr>
              <tr>
                <td>8:35 - 9:00</td>
                <td>Surya Ganguli: <em>Learning deep generative models by
                reversing diffusion</em></td>
              </tr>
              <tr>
                <td>9:00 - 9:15</td>
                <td>
                Guillaume Dehaene:
                <em>Expectation Propagation performs a smoothed gradient descent</em>
                </td>
              </tr>
              <tr>
                <td>9:15 - 9:40</td>
                <td>
                Matthew Johnson:
                <em>Learning representations that support efficient
                inference</em>
                </td>
              </tr>
              <tr>
                <td>9:40 - 10:30</td>
                <td><strong>Panel</strong>: <em>Software</em>
                <br>
                Aki Vehtari, Daniel Ritchie, Dustin Tran, Ryan
                Sepassi, Michael Hughes
                <br>
                Moderator: Trevor Campbell
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Session 2</h3>
          <h4>Chair: Stephan Mandt</h4>
          <table class="table">
            <tbody>
              <tr>
                <td width="20%">11:00 - 11:15</td>
                <td>
                James McInerney:
                <em>B3O: Bayes Empirical Bayes by Bayesian Optimization</em>
                </td>
              </tr>
              <tr>
                <td width="20%">11:15 - 11:35</td>
                <td>Poster spotlights
                <ul>
                Matthew Hoffman:
                <em>ELBO surgery: yet another way to carve up the
                variational evidence lower bound</em>
                <!--<a href="./Hoffman2016.pdf"              -->
                <!--class="btn btn-default btn-xs">Slides</a>-->
                <br>
                Volodymyr Kuleshov:
                <em>Neural Variational Random Field Learning</em>
                <a href="./Kuleshov2016.pdf"
                class="btn btn-default btn-xs">Slides</a>
                <br>
                Jaan Altosaar:
                <em>Proximity Variational Inference</em>
                <a href="./Altosaar2016.pdf"
                class="btn btn-default btn-xs">Slides</a>
                <br>
                Qiang Liu:
                <em>Stein Variational Gradient Descent: Theory and
                Applications</em>
                <a href="./Liu2016.pdf"
                class="btn btn-default btn-xs">Slides</a>
                <br>
                Hans-Christian Ruiz Euler:
                <em>Smoothing Estimates of Diffusion Processes</em>
                <a href="./Euler2016.pdf"
                class="btn btn-default btn-xs">Slides</a>
                <br>
                David A. Moore:
                <em>Symmetrized Variational Inference</em>
                <a href="./Moore2016.pdf"
                class="btn btn-default btn-xs">Slides</a>
                <br>
                Francisco Ruiz:
                <em>Rejection Sampling Variational Inference</em>
                <a href="./Ruiz2016.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </ul>
                </td>
              </tr>
              <tr>
                <td>11:35 - 1:00</td>
                <td>Poster session</td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Session 3</h3>
          <h4>Chair: Stephan Mandt</h4>
          <table class="table">
            <tbody>
              <tr>
                <td width="20%">2:10 - 2:35</td>
                <td>
                Barbara Engelhardt:
                <em>Variational inference in high dimensional analyses</em>
                </td>
              </tr>
              <tr>
                <td>2:35 - 3:00</td>
                <td>
                Jeffrey Regier:
                <em>Learning an astronomical catalog of the visible
                universe through scalable Bayesian inference</em>
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Session 4</h3>
          <h4>Chair: James McInerney</h4>
          <table class="table">
            <tbody>
              <tr>
                <td width="20%">3:30 - 3:45</td>
                <td>
                Matt Hoffman:
                <em>Inference and Introspection in Deep Generative Models of Sparse
                Data</em>
                </td>
              </tr>
              <tr>
                <td>3:45 - 4:10</td>
                <td>
                Jonathan Huggins:
                <em>Coresets for Scalable Bayesian Inference</em>
                </td>
              </tr>
              <tr>
                <td>4:10 - 4:25</td>
                <td>
                Matthew Graham:
                <em>Continuously tempered Hamiltonian Monte Carlo</em>
                </td>
              </tr>
              <tr>
                <td>4:25 - 5:30 </td>
                <td><strong>Panel</strong>: <em>On the Foundations and
                Future of Approximate Inference</em>
                <br>
                Ryan Adams, Barbara Engelhardt, Philipp Hennig, Richard
                Turner
                <br>
                Moderator: David Blei
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Abstracts</h3>
          <hr>
          <h4><a href="https://ganguli-gang.stanford.edu/surya.html">Surya Ganguli</a> (Stanford University)</h4>
          <h4>Learning deep generative models by reversing diffusion</h4>
          <p>
          <strong>Abstract</strong>.
          A large variety of methods have been developed to train deep
          generative models of complex datasets. A common ingredient
          in these methods involves joint training of a recognition
          model that converts complex data distributions into simple
          ones, and a generative model that converts the simple
          distribution into the complex data distribution.  Here we
          discuss a particularly simple approach, inspired from
          non-equilibrium thermodynamics, in which the recognition
          model diffusively destroys structure in data, and the
          generative model is a deep neural network.  This leads to a
          framework in which neural networks essentially reverse the
          arrow of time in an entropy generating diffusive process.
          This combined framework allows for learning highly flexible
          families of probability distributions in which learning,
          sampling, inference, and evaluation are still analytically
          or computationally tractable.  Moreover, the simplicity of
          the recognition process makes this framework an ideal
          starting point for the theoretical analysis of unsupervised
          learning through deep generative modeling.
          <!--<br>                                                         -->
          <!--Joint work with Adel Javanmard, Federico Ricci-Tersenghi, and-->
          <!--Subhabrata Sen.                                              -->
          </p>

          <h4><a href="http://www.cs.princeton.edu/~bee/">Barbara
          Engelhardt</a> (Princeton University)</h4>
          <h4>Variational inference in high dimensional analyses</h4>
          <p>
          <strong>Abstract</strong>.
          High dimensional data requires structured statistical models for appropriate exploratory analyses; however, the associated approximate inference methods often are not robust to random initializations. This talk considers the question of how to combine results from approximate inference methods across specific models to make the results more robust.
          <!--<br>                                                     -->
          <!--Joint work with Dave Blei, Alp Kucukelbir, Stephan Mandt,-->
          <!--James McInerney, and Dustin Tran.                        -->
          </p>

          <h4><a href="http://www.jhhuggins.org">Jonathan Huggins</a> (MIT)</h4>
          <h4>Coresets for Scalable Bayesian Inference</h4>
          <p>
          <strong>Abstract</strong>.
          The use of Bayesian methods in large-scale data settings is attractive
          because of the rich hierarchical models, uncertainty quantification,
          and prior specification they provide. Standard Bayesian inference
          algorithms are computationally expensive, however, making their direct
          application to large datasets difficult or infeasible. Recent work on
          scaling Bayesian inference has focused on modifying the underlying
          algorithms to, for example, use only a random data subsample at each
          iteration. I leverage the insight that data is often redundant to
          instead obtain a weighted subset of the data (called a coreset) that
          is much smaller than the original dataset. One can then use this small
          coreset in any number of existing posterior inference algorithms
          without modification. In this talk, I discuss an efficient coreset
          construction algorithm, which involves calculating a
          likelihood-specific importance distribution over the data, then
          subsampling and re-weighting the data using that distribution. The
          algorithm leads to an approximation of the log-likelihood up to a
          multiplicative error. Crucially for the large-scale data setting, the
          proposed approach permits efficient construction of coresets in both
          streaming and parallel settings. I show how to apply the algorithm to
          construct coresets for Bayesian logistic regression models. I give
          theoretical guarantees on the size and approximation quality of the
          coreset -- both for fixed, known datasets, and in expectation for a
          wide class of data generative models. I demonstrate the efficacy of
          the approach on a number of synthetic and real-world datasets, and
          find that, in practice, the size of the coreset is independent of the
          original dataset size. To conclude, I will discuss shortcomings of the
          multiplicative approximation guarantee provided by the coreset
          construction algorithm and why it is not ideal for the Bayesian
          setting. I propose an alternative approximation guarantee that is
          better suited for obtaining high-quality Bayesian inferences.
          <!--<br>                                                        -->
          <!--Joint work with Alexander G. de G. Matthews, Nicolo Fusi,   -->
          <!--Maurizio Filippone, Neil D. Lawrence, and Zoubin Ghahramani.-->
          </p>

          <h4><a href="http://www.stat.berkeley.edu/~jeff/">Jeffrey Regier</a> (UC Berkeley)</h4>
          <h4>Learning an astronomical catalog of the visible universe through
scalable Bayesian inference</h4>
          <p>
          <strong>Abstract</strong>.
          A central problem in astronomy is to infer locations, colors, and
          other properties of stars and galaxies appearing in astronomical
          images. In the first part of this talk, I present a generative model
          for astronomical images. The number of photons arriving at each pixel
          during an exposure is Poisson distributed, with a rate parameter that
          is a deterministic function of the latent properties of the imaged
          stars and galaxies. A variational Bayes procedure approximates the
          posterior distribution of the latent properties, attaining
          state-of-the-art results. In the second part of the talk, I report on
          scaling the procedure to a 55 TB collection of images. Our
          implementation is written entirely in Julia, a new high-level dynamic
          programming language. Using shared and distributed memory parallelism,
          we demonstrate effective load balancing and scaling on up to 16,384
          Xeon cores on the NERSC Cori supercomputer.
          <!--<br>                                                       -->
          <!--Joint work with Nick Foti, Jason Xu, Dillon Laird, and Alex-->
          <!--Tank.                                                      -->
          </p>

          <h4><a href="http://people.csail.mit.edu/mattjj/">Matthew
          Johnson</a> (Google Brain)</h4>
          <h4>Learning representations that support efficient inference</h4>
          <p>
          <strong>Abstract</strong>.
          Learned inference networks, like those in variational
          autoencoders, provide fast approximate inference in
          flexible, high-capacity models, but can only answer a few
          specific inference queries, and may become data-intensive to
          learn as latent variable models grow more complex.  At
          another extreme, probabilistic graphical models built with
          exponential family structure let us constrain our models so
          as to enable efficient, compositional inference, but models
          built from only these tractable components be too
          restrictive for complex data like images and video. I'll
          compare these approaches and describe one way we can combine
          their strengths, yielding a framework in which neural
          networks help us learn latent variable representations that
          support efficient inference.
          <!--<br>                                         -->
          <!--Joint work with Burak Cakmak and Ole Winther.-->
          </p>
        </div>

      </div>

    </div>

    <hr>

    <footer>
    &nbsp;
    </footer>

  </div>

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/javascript" src="/js/main.js"></script>
</body>
</html>
