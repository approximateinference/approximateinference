<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Dustin Tran">
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <title>Approximate Inference - Schedule</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/main.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>
<body>
  <div class="container">

    <div class="row" style="padding:20px">

      <div class="hidden-xs col-sm-3 col-md-2">
      </div>

      <div class="col-xs-12 col-sm-12 col-md-9">
        <div class="row" style="margin-bottom:-10px;">
          <div class="col-xs-12 hidden-sm hidden-md hidden-lg">
            <a href="http://nips.cc">
              <img src="/img/nips.svg" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
          </div>
          <div class="hidden-xs">
            <a href="http://nips.cc">
              <img src="/img/nips.svg" class="pull-right" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
          </div>
          <h2>Advances in Approximate Bayesian Inference</h2>
          <p class="lead">
            NIPS 2017 Workshop;
            December 8, 2017<br>
            Seaside Ballroom,
            Long Beach Convention Center,
            Long Beach, USA<br>
          </p>
        </div>
      </div>

       <div class="col-xs-12 col-sm-3 col-md-2" id="sidebar" role="navigation">
        <hr>
        <ul class="nav nav-pills nav-stacked">
          <li><a href="../">Home</a></li>
          <li><a href=".">Schedule</a></li>
          <li><a href="../call">Call for Papers</a></li>
          <li><a href="../accepted">Accepted Papers</a></li>
          <li><a href="../program">Program Committee</a></li>
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Organizers</li>
          <li><a href="http://franrruiz.github.io">Francisco Ruiz</a>
          </li>
          <li><a href="http://www.stephanmandt.com">Stephan Mandt</a>
          </li>
          <li><a href="https://cheng-zhang.org">Cheng Zhang</a>
          </li>
          <li><a href="http://jamesmc.com">James McInerney</a>
          </li>
          <li><a href="http://dustintran.com">Dustin Tran</a>
          </li>
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Advisory Committee</li>
          <li><a href="http://tamarabroderick.com">Tamara Broderick</a>
          </li>
          <li><a href="http://www2.aueb.gr/users/mtitsias/">Michalis
          Titsias</a>
          </li>
          <li><a href="http://www.cs.columbia.edu/~blei">David Blei</a>
          </li>
          <li><a href="https://www.ics.uci.edu/~welling/">Max Welling</a>
          </li>
        </ul>
      </div>

      <div class="col-xs-12 col-sm-9 col-md-9">
        <div class="row">
          <hr>
          <p><strong>
          The workshop's recording is available on
          <a href="https://www.youtube.com/playlist?list=PLsatQfvo0v3sUhi3ijRme9MyqwLL5EOiG">
          Youtube</a>.
          </strong></p>
          <h3>Session 1</h3>
          <!-- <h4>Chair: Francisco Ruiz</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="12%">8:30 - 8:35</td>
                <td><strong>Introduction</strong></td>
              </tr>
            </tbody>
          </table>
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="12%">8:35 - 9:00</td>
                <td width="8%">Invited</td>
                <td>
                Iain Murray: <em>Learning priors, likelihoods, or
                posteriors</em>
                <a href="/2017/schedule/Murray2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
              <tr>
                <td>9:00 - 9:15</td>
                <td>Contributed</td>
                <td>
                Josip Djolonga: <em>Learning Implicit Generative
                Models Using Differentiable Graph Tests</em>
                <!-- <a href="/2017/schedule/Djolonga2017.pdf" -->
                <!-- class="btn btn-default btn-xs">Slides</a> -->
                </td>
              </tr>
              <tr>
                <td>9:15 - 9:40</td>
                <td>Invited</td>
                <td>
                Yingzhen Li: <em>Gradient Estimators for Implicit
                Models</em>
                <a href="/2017/schedule/Li2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
              <tr>
                <td>9:40 - 10:00</td>
                <td>Invited</td>
                <td>
                Dawen Liang: <em>Variational Autoencoders for
                Recommendation</em>
                <a href="/2017/schedule/Liang2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">10:00 - 10:30</td>
                <td><strong>Poster Spotlights</strong>
                <!-- <ul>                                                                   -->
                <!-- Zalán Borsos:                                                         -->
                <!-- <em>Variational Inference for DPGMM with Coresets</em>                 -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Marko Järvenpää:                                                    -->
                <!-- <em>Efficient acquisition rules for model-based approximate Bayesian   -->
                <!-- computation</em>                                                       -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Eric Nalisnick:                                                        -->
                <!-- <em>Variational Inference with Stein Mixtures</em>                     -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Francesco Locatello:                                                   -->
                <!-- <em>Boosting Variational Inference: an Optimization Perspective</em>   -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Gabriele Abbati:                                                       -->
                <!-- <em>AdaGeo: Adaptive Geometric Learning for Optimization and           -->
                <!-- Sampling</em>                                                          -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Rachit Singh:                                                          -->
                <!-- <em>Structured Variational Autoencoders for the Beta-Bernoulli         -->
                <!-- Process</em>                                                           -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Rui Luo:                                                               -->
                <!-- <em>Thermostat-assisted continuous-tempered Hamiltonian Monte Carlo    -->
                <!-- for multimodal posterior sampling</em>                     -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Da Tang:                                                               -->
                <!-- <em>Natural Gradients via the Variational Predictive Distribution</em> -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Xiaoyu Lu:                                                             -->
                <!-- <em>On Exploration, Exploitation and Learning in Adaptive Importance   -->
                <!-- Sampling</em>                                                          -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Jonathan Huggins:                                                      -->
                <!-- <em>Generic finite approximations for practical Bayesian</em>          -->
                <!-- nonparametrics</em>                                                    -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Tom Rainforth:                                                         -->
                <!-- <em>Inference Trees: Adaptive Inference with Exploration</em>          -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- <br>                                                                   -->
                <!-- Ari Pakman:                                                            -->
                <!-- <em>Binary Bouncy Particle Sampler</em>                                -->
                <!-- [> <a href="#"                          <]                             -->
                <!-- [> class="btn btn-default btn-xs">#</a> <]                             -->
                <!-- </ul>                                                                  -->
                </td>
              </tr>
              <tr>
                <td width="12%">10:30 - 11:25</td>
                <td><strong>Coffee Break and Poster Session</strong></td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Session 2</h3>
          <!-- <h4>Chair: James McInerney</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="12%">11:25 - 11:45</td>
                <td width="8%">Invited</td>
                <td>
                Cedric Archambeau: <em>Approximate Inference in Industry: Two Applications at Amazon</em>
                <a href="/2017/schedule/Archambeau2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
              <tr>
                <td>11:45 - 12:00</td>
                <td>Contributed</td>
                <td>
                Futoshi Futami: <em>Variational Inference based on
                Robust Divergences</em>
                <a href="/2017/schedule/Futami2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">12:00 - 1:00</td>
                <td><strong>Lunch Break</strong></td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Session 3</h3>
          <!-- <h4>Chair: Cheng Zhang</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="12%">1:00 - 2:05</td>
                <td><strong>Poster Session</strong></td>
              </tr>
            </tbody>
          </table>
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="12%">2:05 - 2:20</td>
                <td width="8%">Contributed</td>
                <td>
                Kira Kempinska: <em>Adversarial Sequential Monte
                Carlo</em>
                <a href="/2017/schedule/Kempinska2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
              <tr>
                <td>2:20 - 2:35</td>
                <td>Contributed</td>
                <td>
                Florian Wenzel: <em>Scalable Logit Gaussian
                Process Classification</em>
                <a href="/2017/schedule/Wenzel2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
              <tr>
                <td>2:35 - 3:00</td>
                <td>Invited</td>
                <td>
                Andreas Damianou: <em>Variational inference in
                deep Gaussian processes</em>
                <a href="/2017/schedule/Damianou2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">3:00 - 3:30</td>
                <td><strong>Coffee Break and Poster Session</strong></td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Session 4</h3>
          <!-- <h4>Chair: Stephan Mandt</h4> -->
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="12%">3:30 - 3:45</td>
                <td width="8%">Contributed</td>
                <td>
                Andrew Miller: <em>Taylor Residual Estimators via
                Automatic Differentiation</em>
                <a href="/2017/schedule/Miller2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
              <tr>
                <td>3:45 - 4:10</td>
                <td>Invited</td>
                <td>
                Antti Honkela: <em>Differential privacy and
                Bayesian learning</em>
                <a href="/2017/schedule/Honkela2017.pdf"
                class="btn btn-default btn-xs">Slides</a>
                </td>
              </tr>
              <tr>
                <td>4:10 - 4:25</td>
                <td>Contributed</td>
                <td>
                Yixin Wang: <em>Frequentist Consistency of
                Variational Bayes</em>
                <!-- <a href="/2017/schedule/Wang2017.pdf"     -->
                <!-- class="btn btn-default btn-xs">Slides</a> -->
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <table class="table">
            <tbody>
              <tr>
                <td width="12%">4:25 - 5:30 </td>
                <td><strong>Panel:</strong> <em>On the Foundations and Future of
                Approximate Inference</em>
                <br>
                Tim Salimans,
                Katherine Heller,
                David Blei,
                Max Welling,
                Zoubin Ghahramani
                <br>
                Moderator: Matt Hoffman
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="row">
          <h3>Abstracts</h3>
          <hr>
          <h4><a href="http://homepages.inf.ed.ac.uk/imurray2">Iain Murray</a>
          (University of Edinburgh)</h4>
          <h4>Learning priors, likelihoods, or posteriors</h4>
          <p>
          <strong>Abstract</strong>.
          As the description of the workshop states: variational and
          Monte Carlo methods are currently the mainstream techniques
          for approximate Bayesian inference. However, we can also
          apply machine learning models to solve inference problems in
          several ways. Firstly, there's no point doing careful
          Bayesian inference if the model is silly. We can represent
          good models, often with hard-to-specify priors or expensive
          likelihoods, with surrogates learned from data. Secondly, we
          can learn how to do inference from experience or simulated
          data. However, this is a workshop, so we can have a friendly
          conversation... There's a huge choice of what to do here,
          and frankly it's often not clear what the best approach is,
          and there are a lot of open theoretical questions. I'll give
          some thoughts, but may raise more questions than answers.
          </p>
          <h4><a href="http://yingzhenh4.net/">Yingzhen Li</a>
          (University of Cambridge)</h4>
          <h4>Gradient Estimators for Implicit Models</h4>
          <p>
          <strong>Abstract</strong>.
          This talk is organised in two parts. First I will start by
          revisiting fundamental tractability issues of Bayesian
          computation and argue that density evaluation of the
          approximate posterior is mostly unnecessary. Then I will
          present one of our recent work on an algorithm for fitting
          implicit posterior distributions. In a nutshell, we proposed
          a gradient estimation method that allow variational
          inference to be applied to those approximate distributions
          without a tractable density.
          </p>
          <!-- <h4><a href="http://www0.cs.ucl.ac.uk/staff/c.archambeau/">Cedric Archambeau</a> -->
          <!-- (Amazon)</h4>                                                                    -->
          <h4><a href="http://dawenl.github.io">Dawen Liang</a>
          (Netflix)</h4>
          <h4>Variational Autoencoders for Recommendation</h4>
          <p>
          <strong>Abstract</strong>.
          In this talk, I will present how we extend variational
          autoencoders (VAEs) to collaborative filtering for implicit
          feedback. We introduce a different regularization parameter
          for the learning objective, which proves to be crucial for
          achieving competitive performance. The resulting model and
          learning algorithm has information-theoretic connections to
          maximum entropy discrimination and the information
          bottleneck principle, as well as many recent work on
          understanding the trade-offs in learning latent variable
          models with VAEs. Empirically, we show that the proposed
          approach significantly outperforms state-of-the-art
          baselines on several real-world datasets. Finally, we
          identify the pros and cons of employing a principled
          Bayesian inference approach and characterize settings where
          it provides the most significant improvements.
          </p>
          <h4><a href="http://adamian.github.io">Andreas
          Damianou</a> (Amazon)</h4>
          <h4>Variational inference in deep Gaussian processes</h4>
          <p>
          <strong>Abstract</strong>.
          Combining deep nets with probabilistic reasoning is
          challenging, because uncertainty needs to be propagated
          across the neural network during inference. This comes in
          addition to the (easier) propagation of gradients. In this
          talk I will talk about a family of variational approximation
          methods developed to tackle the aforementioned computational
          issue in Deep Gaussian processes, which can be seen as
          non-parametric Bayesian neural networks.
          </p>
          <h4><a href="https://www.hiit.fi/u/ahonkela/">Antti
          Honkela</a> (University of Helsinki)</h4>
          <h4>Differential privacy and Bayesian learning</h4>
          <p>
          <strong>Abstract</strong>.
          Differential privacy allows deriving strong privacy
          guarantees for algorithms using private data. In my talk I
          will introduce and review different approaches for
          differentially private Bayesian learning building upon
          different forms of exact and approximate inference.
          </p>

      </div>

    </div>

    <hr>

    <footer>
    &nbsp;
    </footer>

  </div>

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/javascript" src="/js/main.js"></script>
</body>
</html>
